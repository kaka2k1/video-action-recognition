{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3352513,"sourceType":"datasetVersion","datasetId":2022849}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tf-models-official --quiet\n!pip install remotezip tqdm opencv-python einops --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T13:43:37.672243Z","iopub.execute_input":"2024-04-18T13:43:37.672896Z","iopub.status.idle":"2024-04-18T13:45:08.714724Z","shell.execute_reply.started":"2024-04-18T13:43:37.672866Z","shell.execute_reply":"2024-04-18T13:45:08.713578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\n\nimport cv2\nimport einops\nimport pickle\nimport numpy as np\nimport remotezip as rz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport json\nimport keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras import layers\nfrom keras.optimizers import Adam\nfrom keras.losses import SparseCategoricalCrossentropy","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:08.716771Z","iopub.execute_input":"2024-04-18T13:45:08.717107Z","iopub.status.idle":"2024-04-18T13:45:16.548871Z","shell.execute_reply.started":"2024-04-18T13:45:08.717061Z","shell.execute_reply":"2024-04-18T13:45:16.547908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_files_per_class(zip_url):\n  files = []\n  with rz.RemoteZip(URL) as zip:\n    for zip_info in zip.infolist():\n      files.append(zip_info.filename)\n  return files\n\ndef get_class(fname):\n  return fname.split('_')[-3]\n\ndef get_files_per_class(files):\n  files_for_class = collections.defaultdict(list)\n  for fname in files:\n    class_name = get_class(fname)\n    files_for_class[class_name].append(fname)\n  return files_for_class\n\ndef download_from_zip(zip_url, to_dir, file_names):\n  with rz.RemoteZip(zip_url) as zip:\n    for fn in tqdm.tqdm(file_names):\n      class_name = get_class(fn)\n      zip.extract(fn, str(to_dir / class_name))\n      unzipped_file = to_dir / class_name / fn\n\n      fn = pathlib.Path(fn).parts[-1]\n      output_file = to_dir / class_name / fn\n      unzipped_file.rename(output_file,)\n\ndef split_class_lists(files_for_class, count):\n  split_files = []\n  remainder = {}\n  for cls in files_for_class:\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder\n\ndef download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n  files = list_files_per_class(zip_url)\n  for f in files:\n    tokens = f.split('/')\n    if len(tokens) <= 2:\n      files.remove(f) # Remove that item from the list if it does not have a filename\n\n  files_for_class = get_files_per_class(files)\n\n  classes = list(files_for_class.keys())[:num_classes]\n\n  for cls in classes:\n    new_files_for_class = files_for_class[cls]\n    random.shuffle(new_files_for_class)\n    files_for_class[cls] = new_files_for_class\n\n  # Only use the number of classes you want in the dictionary\n  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n\n  dirs = {}\n  for split_name, split_count in splits.items():\n    print(split_name, \":\")\n    split_dir = download_dir / split_name\n    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n    download_from_zip(zip_url, split_dir, split_files)\n    dirs[split_name] = split_dir\n\n  return dirs\n\ndef format_frames(frame, output_size):\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame\n\ndef frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))\n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.549909Z","iopub.execute_input":"2024-04-18T13:45:16.550453Z","iopub.status.idle":"2024-04-18T13:45:16.570130Z","shell.execute_reply.started":"2024-04-18T13:45:16.550427Z","shell.execute_reply":"2024-04-18T13:45:16.569105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nclass FrameGenerator:\n    def __init__(self, path, n_frames, n_classes, training = False):\n#     def __init__(self, path, n_frames, training = False):\n        self.path = path\n        self.n_frames = n_frames\n        self.n_classes = n_classes\n        self.training = training\n        self.class_names = os.listdir(path)\n        #     self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n\n    def get_all_avi(self, root_dir, classes):\n        lst_avi = []\n        lst_cls = []\n        # Iterate over each subfolder in the root directory\n        for folder_name in classes:\n            folder_path = os.path.join(root_dir, folder_name)\n\n            # Check if the item in the root directory is a directory\n            if os.path.isdir(folder_path):\n                # Iterate over each file in the subfolder\n                for file_name in os.listdir(folder_path):\n                    file_path = os.path.join(folder_path, file_name)\n\n                    # Check if the file is an AVI file\n                    if file_name.endswith('.mp4'):\n                        # Print the directory of the AVI file\n                        lst_avi.append(file_path)\n                        lst_cls.append(folder_name)\n#                         print(file_path)\n        return lst_avi, lst_cls\n    \n    def get_files_and_class_names(self):\n        classes = self.class_names[:self.n_classes]\n#         video_paths = self.get_all_avi(self.path, classes)\n        video_paths, lst_classes = self.get_all_avi(self.path, classes)\n        return video_paths, lst_classes\n\n    def __call__(self):\n        video_paths, classes = self.get_files_and_class_names()\n\n        pairs = list(zip(video_paths, classes))\n\n        if self.training:\n            random.shuffle(pairs)\n\n        for path, name in pairs:\n            video_frames = frames_from_video_file(path, self.n_frames)\n            label = self.class_ids_for_name[name] # Encode labels\n            yield video_frames, label","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.573204Z","iopub.execute_input":"2024-04-18T13:45:16.573561Z","iopub.status.idle":"2024-04-18T13:45:16.615903Z","shell.execute_reply.started":"2024-04-18T13:45:16.573529Z","shell.execute_reply":"2024-04-18T13:45:16.615084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\ndownload_dir = pathlib.Path('./UCF101_subset/')\n# batch_size = 8\n# num_frames = 16\noutput_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n                    tf.TensorSpec(shape = (), dtype = tf.int16))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.617036Z","iopub.execute_input":"2024-04-18T13:45:16.617381Z","iopub.status.idle":"2024-04-18T13:45:16.628257Z","shell.execute_reply.started":"2024-04-18T13:45:16.617350Z","shell.execute_reply":"2024-04-18T13:45:16.627374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the dimensions of one frame in the set of frames created\nHEIGHT = 224\nWIDTH = 224","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.629282Z","iopub.execute_input":"2024-04-18T13:45:16.629538Z","iopub.status.idle":"2024-04-18T13:45:16.640719Z","shell.execute_reply.started":"2024-04-18T13:45:16.629516Z","shell.execute_reply":"2024-04-18T13:45:16.640010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv2Plus1D(keras.layers.Layer):\n  def __init__(self, filters, kernel_size, padding):\n    \"\"\"\n      A sequence of convolutional layers that first apply the convolution operation over the\n      spatial dimensions, and then the temporal dimension.\n    \"\"\"\n    super().__init__()\n    self.seq = keras.Sequential([\n        # Spatial decomposition\n        layers.Conv3D(filters=filters,\n                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n                      padding=padding),\n        # Temporal decomposition\n        layers.Conv3D(filters=filters,\n                      kernel_size=(kernel_size[0], 1, 1),\n                      padding=padding)\n        ])\n\n  def call(self, x):\n    return self.seq(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.641780Z","iopub.execute_input":"2024-04-18T13:45:16.642027Z","iopub.status.idle":"2024-04-18T13:45:16.650957Z","shell.execute_reply.started":"2024-04-18T13:45:16.642006Z","shell.execute_reply":"2024-04-18T13:45:16.650238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualMain(keras.layers.Layer):\n    \"\"\"\n    Residual block of the model with convolution, layer normalization, and the\n    activation function, ReLU.\n  \"\"\"\n    def __init__(self, filters, kernel_size):\n        super().__init__()\n        self.seq = keras.Sequential([\n            Conv2Plus1D(filters=filters,\n                        kernel_size=kernel_size,\n                        padding='same'),\n            layers.LayerNormalization(),\n            layers.ReLU(),\n            Conv2Plus1D(filters=filters,\n                        kernel_size=kernel_size,\n                        padding='same'),\n            layers.LayerNormalization()\n        ])\n\n    def call(self, x):\n        return self.seq(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.651906Z","iopub.execute_input":"2024-04-18T13:45:16.652165Z","iopub.status.idle":"2024-04-18T13:45:16.661988Z","shell.execute_reply.started":"2024-04-18T13:45:16.652143Z","shell.execute_reply":"2024-04-18T13:45:16.661175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Project(keras.layers.Layer):\n    \"\"\"\n    Project certain dimensions of the tensor as the data is passed through different\n    sized filters and downsampled.\n  \"\"\"\n    def __init__(self, units):\n        super().__init__()\n        self.seq = keras.Sequential([\n            layers.Dense(units),\n            layers.LayerNormalization()\n        ])\n\n    def call(self, x):\n        return self.seq(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.663147Z","iopub.execute_input":"2024-04-18T13:45:16.663829Z","iopub.status.idle":"2024-04-18T13:45:16.671344Z","shell.execute_reply.started":"2024-04-18T13:45:16.663805Z","shell.execute_reply":"2024-04-18T13:45:16.670500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_residual_block(input, filters, kernel_size):\n    \"\"\"\n    Add residual blocks to the model. If the last dimensions of the input data\n    and filter size does not match, project it such that last dimension matches.\n  \"\"\"\n    out = ResidualMain(filters,\n                     kernel_size)(input)\n\n    res = input\n    # Using the Keras functional APIs, project the last dimension of the tensor to\n    # match the new filter size\n    if out.shape[-1] != input.shape[-1]:\n        res = Project(out.shape[-1])(res)\n\n    return layers.add([res, out])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.674563Z","iopub.execute_input":"2024-04-18T13:45:16.674901Z","iopub.status.idle":"2024-04-18T13:45:16.683440Z","shell.execute_reply.started":"2024-04-18T13:45:16.674876Z","shell.execute_reply":"2024-04-18T13:45:16.682700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResizeVideo(keras.layers.Layer):\n    def __init__(self, height, width):\n        super().__init__()\n        self.height = height\n        self.width = width\n        self.resizing_layer = layers.Resizing(self.height, self.width)\n\n    def call(self, video):\n        \"\"\"\n      Use the einops library to resize the tensor.\n\n      Args:\n        video: Tensor representation of the video, in the form of a set of frames.\n\n      Return:\n        A downsampled size of the video according to the new height and width it should be resized to.\n    \"\"\"\n    # b stands for batch size, t stands for time, h stands for height,\n    # w stands for width, and c stands for the number of channels.\n        old_shape = einops.parse_shape(video, 'b t h w c')\n        images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n        images = self.resizing_layer(images)\n        videos = einops.rearrange(\n            images, '(b t) h w c -> b t h w c',\n            t = old_shape['t'])\n        return videos","metadata":{"execution":{"iopub.status.busy":"2024-04-18T13:45:16.684495Z","iopub.execute_input":"2024-04-18T13:45:16.684828Z","iopub.status.idle":"2024-04-18T13:45:16.694036Z","shell.execute_reply.started":"2024-04-18T13:45:16.684795Z","shell.execute_reply":"2024-04-18T13:45:16.693216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Số lượng lớp để huấn luyện mô hình\nnum_classes_list = list(range(30, 46, 5))\n\n# Số lượng khung hình\nn_frames = 10\n\n# Kích thước batch\nbatch_size = 8\n\n# Đường dẫn tải xuống\nsubset_paths_train = \"/kaggle/input/kinetics-train-5per/kinetics600_5per/kinetics600_5per/train\"\nsubset_paths_val = \"/kaggle/input/kinetics-train-5per/kinetics400_5per/kinetics400_5per/train\"\nnum_classes = 50\ncoeff = 1\n\n# Tạo một tệp tin để lưu các thông số\nfor num_classes in num_classes_list:\n    with open(f'train_{num_classes}_classes_200ep.txt', 'w') as f:\n        print(f\"-------------------- {num_classes} classes --------------------\")\n\n        # Tạo tập dữ liệu huấn luyện, xác thực và kiểm tra\n        train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths_train, n_frames, num_classes, training=True),\n                                                  output_signature = output_signature)\n    #     print\n        train_ds = train_ds.batch(batch_size)\n\n        val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths_val, n_frames, num_classes),\n                                                output_signature = output_signature)\n        val_ds = val_ds.batch(batch_size)\n\n    #         test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], n_frames),\n    #                                                  output_signature = output_signature)\n    #         test_ds = test_ds.batch(batch_size)\n        input_shape = (None, 10, HEIGHT, WIDTH, 3)\n        input = layers.Input(shape=(input_shape[1:]))\n        x = input\n\n        x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        x = ResizeVideo(HEIGHT // (coeff*2), WIDTH // (coeff*2))(x)\n\n        # Block 1\n        x = add_residual_block(x, 16, (3, 3, 3))\n        x = ResizeVideo(HEIGHT // (coeff*4), WIDTH // (coeff*4))(x)\n\n        # Block 2\n        x = add_residual_block(x, 32, (3, 3, 3))\n        x = ResizeVideo(HEIGHT // (coeff*8), WIDTH // (coeff*8))(x)\n\n        # Block 3\n        x = add_residual_block(x, 64, (3, 3, 3))\n        x = ResizeVideo(HEIGHT // (coeff*16), WIDTH // (coeff*16))(x)\n\n        x = add_residual_block(x, 128, (3, 3, 3))\n    #     x = ResizeVideo(HEIGHT // (coeff*32), WIDTH // (coeff*32))(x)\n    #     x = add_residual_block(x, 256, (3, 3, 3))\n\n        x = layers.GlobalAveragePooling3D()(x)\n        x = layers.Flatten()(x)\n        x = layers.Dense(num_classes)(x)\n\n        model = keras.Model(input, x)\n        # Xây dựng và biên dịch mô hình\n        frames, label = next(iter(train_ds))\n        model.build(frames)\n        optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n\n        model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                      optimizer=optimizer,\n                      metrics = ['accuracy'])\n\n        # Huấn luyện mô hình\n        history = model.fit(x = train_ds,\n                            epochs = 50,\n                            validation_data = train_ds)\n\n        # Lưu các thông số vào tệp tin\n        f.write(json.dumps(history.history))\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:07:49.661946Z","iopub.execute_input":"2024-04-18T15:07:49.662399Z","iopub.status.idle":"2024-04-18T15:38:58.269852Z","shell.execute_reply.started":"2024-04-18T15:07:49.662363Z","shell.execute_reply":"2024-04-18T15:38:58.267141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}